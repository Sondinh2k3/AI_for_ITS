# PPO Training - TÃ³m Táº¯t Nhanh

## ğŸ“¦ CÃ¡c File thuáº­t toÃ¡n PPO

```
scripts/
â”œâ”€â”€ train_ppo.py              â† Script training PPO chÃ­nh
â”œâ”€â”€ eval_ppo.py               â† Script Ä‘Ã¡nh giÃ¡ model
â”œâ”€â”€ ppo_config_examples.py    â† VÃ­ dá»¥ cáº¥u hÃ¬nh (5 scenarios)
â”œâ”€â”€ run_training.sh           â† Script menu tÆ°Æ¡ng tÃ¡c
â””â”€â”€ README_PPO.md             â† HÆ°á»›ng dáº«n nhanh

TRAINING_GUIDE.md             â† HÆ°á»›ng dáº«n chi tiáº¿t Ä‘áº§y Ä‘á»§
```

## Báº¯t Äáº§u 

### 1. Setup (Láº§n Ä‘áº§u)

```bash
# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng
source .venv/bin/activate

# CÃ i Ä‘áº·t dependencies
pip install -e .

# Kiá»ƒm tra
python -c "import ray; print('âœ“')"
```

### 2. Training Nhanh

```bash
# Test nhanh (10 iterations)
python scripts/train_ppo.py --iterations 10 --workers 1

# Training tiÃªu chuáº©n (100 iterations)
python scripts/train_ppo.py --network grid4x4 --iterations 100 --workers 2

# Training cao cáº¥p (500 iterations, GPU)
python scripts/train_ppo.py --network zurich --iterations 500 --workers 4 --gpu
```

### 3. ÄÃ¡nh GiÃ¡

```bash
# Sau training, copy Ä‘Æ°á»ng dáº«n checkpoint
CKPT="./results/ppo_grid4x4_.../checkpoint_000050"

# ÄÃ¡nh giÃ¡
python scripts/eval_ppo.py --checkpoint $CKPT --episodes 5
```

## LÃµi Thuáº­t toÃ¡n PPO

**CÃ´ng thá»©c:**
```
L^CLIP(Î¸) = E_t [ min(r_t(Î¸) * Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * Ã‚_t) ]
```

**Æ¯u Ä‘iá»ƒm:**
- á»”n Ä‘á»‹nh, dá»… tune
- Hiá»‡u quáº£ sample
- Hoáº¡t Ä‘á»™ng tá»‘t cho traffic control

**Luá»“ng:**
1. Rollout: Cháº¡y environment vá»›i policy hiá»‡n táº¡i
2. Compute: TÃ­nh advantages & returns (GAE)
3. Update: Multiple SGD passes trÃªn batch
4. Repeat: Äáº¿n converge

## Tham Sá»‘ ChÃ­nh

| Tham sá»‘ | Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|----------|-------|
| `learning_rate` | 5e-5 | Tá»‘c Ä‘á»™ há»c |
| `entropy_coeff` | 0.01 | Khuyáº¿n khÃ­ch explore |
| `clip_param` | 0.3 | PPO clip range |
| `gamma` | 0.99 | Discount factor |
| `workers` | 2 | Parallel collection |

## Káº¿t Quáº£

```
results/
â””â”€â”€ ppo_grid4x4_20250104_120000/
    â”œâ”€â”€ checkpoint_000010/
    â”œâ”€â”€ checkpoint_000020/
    â”œâ”€â”€ training_config.json
    â””â”€â”€ progress.csv
```

## Customize

Äá»ƒ thay Ä‘á»•i tham sá»‘, sá»­a trong `train_ppo.py`:

```python
def create_ppo_config(...):
    config = (
        PPOConfig()
        .training(
            lr=1e-4,              # â† Thay learning rate
            entropy_coeff=0.05,   # â† Thay entropy
            ...
        )
    )
```

Hoáº·c dÃ¹ng examples tá»« `ppo_config_examples.py`:
- `get_ppo_config_small()` - Test nhanh
- `get_ppo_config_medium()` - Production (máº·c Ä‘á»‹nh)
- `get_ppo_config_large()` - High performance
- `get_ppo_config_exploration()` - KhÃ¡m phÃ¡ cao
- `get_ppo_config_stable()` - á»”n Ä‘á»‹nh cao

## Troubleshooting

| Problem | Solution |
|---------|----------|
| "SUMO_HOME not set" | `export SUMO_HOME=/usr/share/sumo` |
| Out of memory | Giáº£m workers: `--workers 1` |
| Training quÃ¡ cháº­m | TÄƒng workers hoáº·c dÃ¹ng GPU |
| Model khÃ´ng há»c | TÄƒng entropy_coeff hoáº·c learning_rate |

## TÃ i Liá»‡u

- **Chi tiáº¿t:** `TRAINING_GUIDE.md`
- **Nhanh:** `scripts/README_PPO.md`
- **VÃ­ dá»¥:** `scripts/ppo_config_examples.py`

---

**KhÃ´ng cÃ³ thay Ä‘á»•i nÃ o trong:**
- `src/environment/drl_algo/` (Environment)
- `pyproject.toml` (Dependencies)

**CÃ¡c file má»›i:**
- âœ¨ `scripts/train_ppo.py` - Script training chÃ­nh
- âœ¨ `scripts/eval_ppo.py` - Script evaluation
- âœ¨ `scripts/ppo_config_examples.py` - Config examples
- âœ¨ `scripts/run_training.sh` - Menu tÆ°Æ¡ng tÃ¡c
- âœ¨ `TRAINING_GUIDE.md` - HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§
