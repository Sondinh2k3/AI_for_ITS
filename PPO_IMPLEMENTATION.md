# ğŸ“š PPO Training Implementation - HoÃ n Táº¥t

DÆ°á»›i Ä‘Ã¢y lÃ  tÃ³m táº¯t nhá»¯ng gÃ¬ Ä‘Ã£ Ä‘Æ°á»£c táº¡o Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh PPO (Proximal Policy Optimization) cho Ä‘iá»u khiá»ƒn Ä‘Ã¨n giao thÃ´ng thÃ­ch á»©ng trong SUMO.

## âœ¨ CÃ¡c File ÄÃ£ Táº¡o

### 1. **Script Training ChÃ­nh** 
ğŸ“„ `scripts/train_ppo.py` (350+ dÃ²ng)
- Huáº¥n luyá»‡n PPO agents trÃªn SUMO environment
- Há»— trá»£ multiple networks, workers, GPU
- LÆ°u checkpoints, cáº¥u hÃ¬nh, vÃ  káº¿t quáº£
- TÃ¹y chá»‰nh learning rate, entropy, clip parameter, v.v.
- Custom stopper dá»±a trÃªn iterations hoáº·c reward threshold

**TÃ­nh nÄƒng chÃ­nh:**
- âœ… Multi-worker parallel training
- âœ… GPU support
- âœ… Checkpoint management
- âœ… Real-time monitoring
- âœ… Configurable hyperparameters

### 2. **Script Evaluation**
ğŸ“„ `scripts/eval_ppo.py` (170+ dÃ²ng)
- ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh Ä‘Ã£ train
- Há»— trá»£ multi-agent vÃ  single-agent
- Lá»±a chá»n GUI rendering
- Thá»‘ng kÃª performance (reward, length)

### 3. **Config Examples**
ğŸ“„ `scripts/ppo_config_examples.py` (180+ dÃ²ng)
- 5 cáº¥u hÃ¬nh PPO pre-defined:
  1. Small (quick testing)
  2. Medium (production - máº·c Ä‘á»‹nh)
  3. Large (high performance)
  4. Exploration-focused (hard problems)
  5. Stability-focused (convergence)
- Hyperparameter tuning guide

### 4. **Interactive Menu Script**
ğŸ“„ `scripts/run_training.sh` (bash)
- Menu tÆ°Æ¡ng tÃ¡c Ä‘á»ƒ training/evaluation
- Dá»… sá»­ dá»¥ng cho users khÃ´ng quen command line

### 5. **TÃ i Liá»‡u Chi Tiáº¿t**
ğŸ“„ `TRAINING_GUIDE.md` (400+ dÃ²ng)
- HÆ°á»›ng dáº«n Ä‘áº§y Ä‘á»§ tá»« cÃ i Ä‘áº·t Ä‘áº¿n káº¿t quáº£
- Cáº¥u trÃºc file, vÃ­ dá»¥, troubleshooting
- Giáº£i thÃ­ch thuáº­t toÃ¡n PPO
- Tham sá»‘ cáº¥u hÃ¬nh chi tiáº¿t

### 6. **TÃ³m Táº¯t Nhanh**
ğŸ“„ `PPO_SUMMARY.md`
- Quick start guide
- CÃ¡c lá»‡nh thÆ°á»ng dÃ¹ng
- Troubleshooting nhanh

### 7. **HÆ°á»›ng Dáº«n Nhanh**
ğŸ“„ `scripts/README_PPO.md`
- LÃµi thuáº­t toÃ¡n PPO
- Workflow thÆ°á»ng ngÃ y
- Monitoring training

### 8. **Verification Script**
ğŸ“„ `verify_setup.py` (250+ dÃ²ng)
- Kiá»ƒm tra environment, packages, files
- Gá»£i Ã½ cÃ¡ch fix náº¿u cÃ³ lá»—i
- Cháº¡y trÆ°á»›c khi training

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### BÆ°á»›c 1: Kiá»ƒm tra Setup
```bash
python verify_setup.py
```

### BÆ°á»›c 2: Training
```bash
# Quick test
python scripts/train_ppo.py --iterations 10 --workers 1

# Standard training
python scripts/train_ppo.py --network grid4x4 --iterations 100 --workers 2

# Full training with GPU
python scripts/train_ppo.py --network zurich --iterations 500 --workers 4 --gpu
```

### BÆ°á»›c 3: Evaluation
```bash
python scripts/eval_ppo.py --checkpoint ./results/ppo_grid4x4_.../checkpoint_000050 --episodes 5
```

---

## ğŸ§  LÃµi Thuáº­t ToÃ¡n PPO

**CÃ´ng thá»©c Clipped PPO:**
```
L^CLIP(Î¸) = E_t [ min(r_t(Î¸) * Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * Ã‚_t) ]

Trong Ä‘Ã³:
  r_t(Î¸) = Ï€Î¸(at|st) / Ï€Î¸_old(at|st)     [Probability Ratio]
  Ã‚_t = Q(st, at) - V(st)              [Advantage Estimate]
  Îµ = clip_param (thÆ°á»ng 0.3)           [Clip Range]
```

**Táº¡i sao PPO tá»‘t cho bÃ i toÃ¡n nÃ y?**
- âœ… á»”n Ä‘á»‹nh hÆ¡n policy gradient methods
- âœ… Hiá»‡u quáº£ máº«u (sample-efficient)
- âœ… Dá»… implement vÃ  debug
- âœ… Hoáº¡t Ä‘á»™ng tá»‘t vá»›i continuous actions
- âœ… Hoáº¡t Ä‘á»™ng tá»‘t vá»›i multi-agent

**Luá»“ng Training:**
```
1. ROLLOUT PHASE
   â””â”€ Cháº¡y environment, collect trajectories

2. ADVANTAGE ESTIMATION
   â””â”€ TÃ­nh GAE (Generalized Advantage Estimation)

3. MULTIPLE UPDATE PHASES
   â”œâ”€ Shuffle trajectories
   â”œâ”€ Mini-batch gradient descent
   â”œâ”€ Calculate clipped PPO loss
   â”œâ”€ Backward pass
   â””â”€ Update parameters

4. REPEAT (n iterations)
```

---

## âš™ï¸ Tham Sá»‘ ChÃ­nh

### Environment Parameters
| Tham sá»‘ | GiÃ¡ trá»‹ | MÃ´ táº£ |
|---------|--------|-------|
| `max_green` | 60s | Thá»i gian xanh tá»‘i Ä‘a |
| `min_green` | 5s | Thá»i gian xanh tá»‘i thiá»ƒu |
| `delta_time` | 5s | Khoáº£ng thá»i gian action |
| `yellow_time` | 3s | Thá»i gian vÃ ng |

### PPO Hyperparameters
| Tham sá»‘ | GiÃ¡ trá»‹ | TÃ¡c Dá»¥ng |
|---------|--------|---------|
| `lr` | 5e-5 | Tá»‘c Ä‘á»™ há»c |
| `gamma` | 0.99 | Discount factor (long-term) |
| `lambda` | 0.95 | GAE lambda (bias-var tradeoff) |
| `entropy_coeff` | 0.01 | Khuyáº¿n khÃ­ch exploration |
| `clip_param` | 0.3 | PPO clipping range |
| `train_batch_size` | 4096 | Batch size má»—i update |
| `sgd_minibatch_size` | 128 | Mini-batch size |
| `num_sgd_iter` | 30 | Sá»‘ epochs má»—i iteration |
| `fcnet_hiddens` | [256, 256] | Hidden layer sizes |

---

## ğŸ“Š Output Structure

```
results/
â””â”€â”€ ppo_grid4x4_20250104_120000/         [timestamp]
    â”œâ”€â”€ checkpoint_000010/
    â”‚   â”œâ”€â”€ algorithm_state.pkl          [Model state]
    â”‚   â”œâ”€â”€ policy_0/
    â”‚   â”‚   â”œâ”€â”€ model.pkl                [Neural network weights]
    â”‚   â”‚   â””â”€â”€ rllib_checkpoint.json
    â”‚   â””â”€â”€ training_iteration
    â”œâ”€â”€ checkpoint_000020/
    â”œâ”€â”€ checkpoint_000050/               [Best checkpoint]
    â”œâ”€â”€ training_config.json             [Cáº¥u hÃ¬nh training]
    â””â”€â”€ progress.csv                     [Metrics má»—i iteration]
```

**training_config.json:**
```json
{
  "experiment_name": "ppo_grid4x4_20250104_120000",
  "network_name": "grid4x4",
  "num_iterations": 100,
  "num_workers": 2,
  "checkpoint_interval": 10,
  "use_gpu": false,
  "seed": 42,
  "best_checkpoint": "./results/.../checkpoint_000050",
  "best_reward": 125.45
}
```

---

## âœ… KhÃ´ng CÃ³ Thay Äá»•i NÃ o Trong

- âœ“ `src/environment/drl_algo/env.py` - Environment khÃ´ng Ä‘á»•i
- âœ“ `src/environment/drl_algo/traffic_signal.py` - Traffic signal logic khÃ´ng Ä‘á»•i
- âœ“ `src/environment/drl_algo/observations.py` - Observations khÃ´ng Ä‘á»•i
- âœ“ `src/environment/drl_algo/resco_envs.py` - RESCO envs khÃ´ng Ä‘á»•i
- âœ“ `pyproject.toml` - Dependencies khÃ´ng Ä‘á»•i

---

## ğŸ“‹ Workflow ThÆ°á»ng NgÃ y

### Phase 1: Development
```bash
# Test nhanh, debug issues
python scripts/train_ppo.py --iterations 10 --workers 1
```

### Phase 2: Validation
```bash
# Kiá»ƒm tra xem model há»c Ä‘Æ°á»£c khÃ´ng
python scripts/train_ppo.py --network grid4x4 --iterations 50 --workers 2
```

### Phase 3: Training
```bash
# Cháº¡y production training, cÃ³ thá»ƒ background
nohup python scripts/train_ppo.py \
  --network zurich \
  --iterations 500 \
  --workers 4 \
  --gpu &
```

### Phase 4: Evaluation
```bash
# ÄÃ¡nh giÃ¡ trÃªn test set
python scripts/eval_ppo.py \
  --checkpoint ./results/.../checkpoint_000100 \
  --episodes 20
```

### Phase 5: Analysis
```bash
# Xem káº¿t quáº£
cat results/ppo_*/training_config.json
ls -lh results/ppo_*/checkpoint_*
```

---

## ğŸ”§ Customization

### Thay Äá»•i Learning Rate
Sá»­a trong `train_ppo.py`, hÃ m `create_ppo_config()`:
```python
.training(
    lr=1e-4,  # â† Thay tá»« 5e-5 thÃ nh 1e-4
)
```

### Thay Äá»•i Network Size
```python
.training(
    model={
        "fcnet_hiddens": [512, 512],  # â† Lá»›n hÆ¡n
    },
)
```

### TÄƒng Exploration
```python
.training(
    entropy_coeff=0.05,  # â† Tá»« 0.01 lÃªn 0.05
)
```

---

## ğŸ“ˆ Expected Results

Sau training thÃ nh cÃ´ng, báº¡n sáº½ tháº¥y:

1. **Reward tÄƒng dáº§n:**
   ```
   Iteration   1: Episode Reward Mean: -12.45
   Iteration  10: Episode Reward Mean:  25.67
   Iteration  50: Episode Reward Mean:  85.34
   Iteration 100: Episode Reward Mean: 125.45
   ```

2. **Episode length á»•n Ä‘á»‹nh** hoáº·c tÄƒng (Ä‘Ã³ lÃ  tá»‘t!)

3. **Checkpoint lÆ°u Ä‘Æ°á»£c** má»—i 10 iterations

4. **Config file Ä‘Æ°á»£c táº¡o** vá»›i best checkpoint info

---

## ğŸ†˜ Troubleshooting

| Problem | Solution |
|---------|----------|
| `ModuleNotFoundError: ray` | `pip install -e .` |
| `SUMO_HOME not set` | `export SUMO_HOME=/usr/share/sumo` |
| Out of Memory | `--workers 1` hoáº·c giáº£m batch size |
| Training quÃ¡ cháº­m | TÄƒng workers hoáº·c `--gpu` |
| Model khÃ´ng há»c | TÄƒng `entropy_coeff` hoáº·c `lr` |
| Network files not found | Kiá»ƒm tra path trong `network/` |

---

## ğŸ“š TÃ i Liá»‡u

| File | Ná»™i Dung |
|------|---------|
| `TRAINING_GUIDE.md` | HÆ°á»›ng dáº«n chi tiáº¿t (400+ lines) |
| `PPO_SUMMARY.md` | Quick start summary |
| `scripts/README_PPO.md` | Workflow & monitoring |
| `scripts/ppo_config_examples.py` | Config examples |
| `verify_setup.py` | Setup verification |

---

## ğŸ“ Há»c Táº­p ThÃªm

**PPO Paper:**
- https://arxiv.org/abs/1707.06347

**Ray RLlib Docs:**
- https://docs.ray.io/en/latest/rllib/

**Traffic Control Papers:**
- Deep RL for Traffic Signal Control
- Multi-agent Traffic Control

---

## ğŸ“ Notes

1. âœ“ Script nÃ y **khÃ´ng thay Ä‘á»•i environment code**
2. âœ“ Táº¥t cáº£ hyperparameters Ä‘á»u **cÃ³ thá»ƒ customize**
3. âœ“ Training results Ä‘Æ°á»£c **lÆ°u tá»± Ä‘á»™ng** vá»›i timestamp
4. âœ“ Models cÃ³ thá»ƒ **load láº¡i** Ä‘á»ƒ tiáº¿p tá»¥c training
5. âœ“ Há»— trá»£ **GPU** Ä‘á»ƒ train nhanh hÆ¡n

---

**ChÃºc báº¡n huáº¥n luyá»‡n thÃ nh cÃ´ng!** ğŸš€

NgÃ y táº¡o: 2025
