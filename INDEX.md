# ğŸ¯ PPO Training for Adaptive Traffic Signal Control - Complete Package

## ğŸ“¦ Táº¥t cáº£ cÃ¡c file Ä‘Ã£ Ä‘Æ°á»£c táº¡o

### Main Training Scripts (scripts/)

| File | KÃ­ch thÆ°á»›c | MÃ´ táº£ |
|------|-----------|-------|
| `train_ppo.py` | 13 KB | **Script training PPO chÃ­nh** - Huáº¥n luyá»‡n agents |
| `eval_ppo.py` | 5.5 KB | **Script evaluation** - ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh |
| `ppo_config_examples.py` | 6.7 KB | **Config examples** - 5 cáº¥u hÃ¬nh PPO pre-defined |
| `run_training.sh` | - | **Menu tÆ°Æ¡ng tÃ¡c** - Bash script cho training interactively |
| `README_PPO.md` | - | Quick start & troubleshooting |

### Documentation Files

| File | KÃ­ch thÆ°á»›c | Ná»™i Dung |
|------|-----------|---------|
| `TRAINING_GUIDE.md` | 12 KB | **ğŸ“š HÆ°á»›ng dáº«n chi tiáº¿t** - CÃ i Ä‘áº·t, training, tuning, troubleshooting |
| `PPO_SUMMARY.md` | 3.7 KB | **âš¡ Quick start** - CÃ¡c lá»‡nh nhanh & tÃ³m táº¯t |
| `PPO_IMPLEMENTATION.md` | 8.9 KB | **ğŸ“‹ HoÃ n tÃ¡t report** - Táº¥t cáº£ táº¡o cÃ¡i gÃ¬, cÃ¡ch sá»­ dá»¥ng, káº¿t quáº£ |
| `README.md` (trong scripts/) | - | HÆ°á»›ng dáº«n nhanh cho scripts/ |

### Verification & Setup

| File | KÃ­ch thÆ°á»›c | MÃ´ Táº£ |
|------|-----------|-------|
| `verify_setup.py` | 6.6 KB | âœ… Kiá»ƒm tra environment, packages, files |

---

## ğŸš€ Quick Start (3 bÆ°á»›c)

### 1ï¸âƒ£ Kiá»ƒm tra Setup
```bash
python verify_setup.py
```

### 2ï¸âƒ£ Training
```bash
# Test nhanh (10 iterations)
python scripts/train_ppo.py --iterations 10 --workers 1

# Production (100 iterations)
python scripts/train_ppo.py --iterations 100 --workers 2

# Full (500 iterations, GPU)
python scripts/train_ppo.py --iterations 500 --workers 4 --gpu
```

### 3ï¸âƒ£ Evaluation
```bash
python scripts/eval_ppo.py \
  --checkpoint ./results/ppo_grid4x4_.../checkpoint_000050 \
  --episodes 5
```

---

## ğŸ“Š CÃ¡c Lá»‡nh ThÆ°á»ng DÃ¹ng

### Training Options
```bash
# Xem táº¥t cáº£ options
python scripts/train_ppo.py --help

# Test nhanh (development)
python scripts/train_ppo.py --iterations 10 --workers 1

# Standard training (production)
python scripts/train_ppo.py --network zurich --iterations 100 --workers 2

# Advanced with GPU
python scripts/train_ppo.py \
  --network PhuQuoc \
  --iterations 500 \
  --workers 4 \
  --checkpoint-interval 20 \
  --gpu \
  --seed 42

# With custom experiment name
python scripts/train_ppo.py \
  --experiment-name "ppo_zurich_experiment1" \
  --reward-threshold 100
```

### Evaluation Options
```bash
# Basic evaluation
python scripts/eval_ppo.py --checkpoint <path>

# With custom network
python scripts/eval_ppo.py \
  --checkpoint <path> \
  --network zurich \
  --episodes 10

# With GUI rendering
python scripts/eval_ppo.py \
  --checkpoint <path> \
  --gui \
  --episodes 3 \
  --max-steps 500
```

### Interactive Menu (Bash)
```bash
bash scripts/run_training.sh
```

---

## ğŸ§  Vá» PPO Algorithm

**CÃ´ng thá»©c Clipped PPO:**
```
L^CLIP(Î¸) = E_t [ min(r_t(Î¸) * Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * Ã‚_t) ]
```

**Táº¡i sao PPO tá»‘t:**
- âœ… Stable learning
- âœ… Sample efficient
- âœ… Easy to implement & debug
- âœ… Works with continuous actions
- âœ… Multi-agent compatible

**Luá»“ng Training:**
1. **Rollout**: Cháº¡y environment, collect trajectories
2. **Advantage**: TÃ­nh GAE (Generalized Advantage Estimation)
3. **Update**: Multiple SGD passes with clipped loss
4. **Repeat**: n iterations

---

## âš™ï¸ Tham Sá»‘ Cáº¥u HÃ¬nh

### Environment (SUMO)
- `max_green`: 60s (max green light duration)
- `min_green`: 5s (min green light duration)  
- `delta_time`: 5s (action interval)
- `yellow_time`: 3s (yellow light duration)

### PPO Training (Defaults)
- `learning_rate`: 5e-5
- `gamma`: 0.99 (discount factor)
- `lambda`: 0.95 (GAE lambda)
- `entropy_coeff`: 0.01 (exploration)
- `clip_param`: 0.3 (PPO clip range)
- `train_batch_size`: 4096
- `fcnet_hiddens`: [256, 256]
- `num_workers`: 2
- `gpu`: False

Äá»ƒ customize, xem `scripts/ppo_config_examples.py` hoáº·c sá»­a `train_ppo.py`

---

## ğŸ“‚ Output Structure

```
results/
â””â”€â”€ ppo_grid4x4_20250104_120000/      [timestamp folder]
    â”œâ”€â”€ checkpoint_000010/             [saved weights]
    â”œâ”€â”€ checkpoint_000020/
    â”œâ”€â”€ checkpoint_000050/             [best]
    â”œâ”€â”€ training_config.json           [config saved]
    â””â”€â”€ progress.csv                   [metrics]
```

---

## ğŸ“š Documentation Roadmap

Tuá»³ vÃ o nhu cáº§u, Ä‘á»c:

1. **Báº¯t Ä‘áº§u ngay**: 
   - â†’ `PPO_SUMMARY.md` (2 phÃºt)
   - â†’ `scripts/README_PPO.md` (5 phÃºt)

2. **CÃ i Ä‘áº·t Ä‘áº§y Ä‘á»§**:
   - â†’ `TRAINING_GUIDE.md` (20 phÃºt)
   - â†’ `verify_setup.py` (1 phÃºt verify)

3. **TÃ¬m hiá»ƒu chi tiáº¿t**:
   - â†’ `PPO_IMPLEMENTATION.md` (30 phÃºt)
   - â†’ `scripts/ppo_config_examples.py` (15 phÃºt)

4. **Troubleshooting**:
   - â†’ `TRAINING_GUIDE.md` (Troubleshooting section)
   - â†’ `verify_setup.py` (auto-check)

---

## âŒ KhÃ´ng CÃ³ Thay Äá»•i NÃ o Trong

âœ… `src/environment/drl_algo/` - Environment code
âœ… `pyproject.toml` - Dependencies
âœ… Existing DQN training scripts

---

## ğŸ” File Structure

```
AI_for_ITS/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ppo.py              âœ¨ NEW - Main training
â”‚   â”œâ”€â”€ eval_ppo.py               âœ¨ NEW - Evaluation
â”‚   â”œâ”€â”€ ppo_config_examples.py    âœ¨ NEW - Config examples
â”‚   â”œâ”€â”€ run_training.sh           âœ¨ NEW - Interactive menu
â”‚   â”œâ”€â”€ README_PPO.md             âœ¨ NEW - Quick guide
â”‚   â””â”€â”€ train_rllib.py            (old - DQN)
â”‚
â”œâ”€â”€ src/environment/drl_algo/
â”‚   â”œâ”€â”€ env.py                    (unchanged)
â”‚   â”œâ”€â”€ traffic_signal.py         (unchanged)
â”‚   â”œâ”€â”€ observations.py           (unchanged)
â”‚   â””â”€â”€ resco_envs.py            (unchanged)
â”‚
â”œâ”€â”€ verify_setup.py               âœ¨ NEW - Verification
â”œâ”€â”€ TRAINING_GUIDE.md             âœ¨ NEW - Detailed guide
â”œâ”€â”€ PPO_SUMMARY.md                âœ¨ NEW - Quick start
â”œâ”€â”€ PPO_IMPLEMENTATION.md         âœ¨ NEW - Full report
â””â”€â”€ network/
    â”œâ”€â”€ grid4x4/
    â”œâ”€â”€ 4x4loop/
    â”œâ”€â”€ zurich/
    â””â”€â”€ PhuQuoc/
```

---

## âœ¨ Key Features

### train_ppo.py
- âœ… Multi-network support (grid4x4, zurich, PhuQuoc, etc.)
- âœ… Parallel workers for faster training
- âœ… GPU support
- âœ… Automatic checkpoint saving
- âœ… Custom stopper (max iterations or reward threshold)
- âœ… Full hyperparameter customization
- âœ… Real-time monitoring
- âœ… Config auto-save

### eval_ppo.py
- âœ… Load trained checkpoints
- âœ… Multi-episode evaluation
- âœ… Per-episode statistics
- âœ… GUI rendering option
- âœ… Flexible max steps

### verify_setup.py
- âœ… Python version check
- âœ… Required packages check
- âœ… SUMO setup verification
- âœ… Project structure verification
- âœ… Network files check
- âœ… Auto-create results directory
- âœ… Clear troubleshooting suggestions

---

## ğŸ“ Learning Resources

### Inside This Package
- `PPO_IMPLEMENTATION.md` - Algorithm explanation
- `scripts/ppo_config_examples.py` - Tuning guide
- Comments in `train_ppo.py` and `eval_ppo.py`

### External Resources
- **PPO Paper**: https://arxiv.org/abs/1707.06347
- **Ray RLlib**: https://docs.ray.io/en/latest/rllib/
- **Gymnasium**: https://gymnasium.farama.org/
- **SUMO**: https://sumo.dlr.de/

---

## ğŸ†˜ Quick Troubleshooting

| Issue | Fix |
|-------|-----|
| `ModuleNotFoundError: ray` | `pip install -e .` |
| `SUMO_HOME not set` | `export SUMO_HOME=/usr/share/sumo` |
| Out of memory | `--workers 1` |
| Too slow | `--workers 4` or `--gpu` |
| Model not learning | Increase `entropy_coeff` in code |
| Files not found | Run `python verify_setup.py` |

---

## ğŸ“ Next Steps

1. **Run verification**: `python verify_setup.py`
2. **Read quick start**: `cat PPO_SUMMARY.md`
3. **Start training**: `python scripts/train_ppo.py --iterations 10 --workers 1`
4. **Check results**: `ls -lh results/`
5. **Evaluate**: `python scripts/eval_ppo.py --checkpoint <path>`

---

## ğŸ“Š Expected Performance

After successful training:
```
Iteration   1: Reward = -12.45
Iteration  10: Reward =  25.67
Iteration  50: Reward =  85.34
Iteration 100: Reward = 125.45
```

(Exact numbers depend on network, seeds, etc.)

---

## ğŸ’¡ Tips for Best Results

1. **Start small**: Test with `--iterations 10 --workers 1` first
2. **Use GPU**: 5-10x faster training with `--gpu`
3. **Monitor**: Watch output during training
4. **Tune gradually**: Change one hyperparameter at a time
5. **Save often**: Checkpoints auto-saved, can resume anytime
6. **Compare**: Keep track of different experiments

---

## ğŸ“ Support

For issues:
1. Run `python verify_setup.py`
2. Check `TRAINING_GUIDE.md` troubleshooting section
3. Read `PPO_IMPLEMENTATION.md` for details
4. Check script help: `python scripts/train_ppo.py --help`

---

**Ready to start training?** ğŸš€

```bash
python verify_setup.py
python scripts/train_ppo.py
```

ChÃºc thÃ nh cÃ´ng! ğŸ‰
