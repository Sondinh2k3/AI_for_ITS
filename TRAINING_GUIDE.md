# PPO Training for Adaptive Traffic Signal Control

HÆ°á»›ng dáº«n chi tiáº¿t vá» huáº¥n luyá»‡n mÃ´ hÃ¬nh PPO (Proximal Policy Optimization) Ä‘á»ƒ Ä‘iá»u khiá»ƒn Ä‘Ã¨n giao thÃ´ng thÃ­ch á»©ng trong SUMO.

## ğŸ“‹ Má»¥c Lá»¥c

1. [Cáº¥u trÃºc File](#cáº¥u-trÃºc-file)
2. [CÃ i Ä‘áº·t MÃ´i trÆ°á»ng](#cÃ i-Ä‘áº·t-mÃ´i-trÆ°á»ng)
3. [Huáº¥n luyá»‡n MÃ´ hÃ¬nh](#huáº¥n-luyá»‡n-mÃ´-hÃ¬nh)
4. [ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh](#Ä‘Ã¡nh-giÃ¡-mÃ´-hÃ¬nh)
5. [Tham sá»‘ Cáº¥u hÃ¬nh](#tham-sá»‘-cáº¥u-hÃ¬nh)
6. [Cáº¥u trÃºc PPO](#cáº¥u-trÃºc-ppo)
7. [Káº¿t quáº£ Huáº¥n luyá»‡n](#káº¿t-quáº£-huáº¥n-luyá»‡n)

---

## ğŸ—‚ï¸ Cáº¥u trÃºc File

```
AI_for_ITS/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ppo.py          # Script huáº¥n luyá»‡n PPO â† Táº O Má»šI
â”‚   â”œâ”€â”€ eval_ppo.py           # Script Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh â† Táº O Má»šI
â”‚   â””â”€â”€ train_rllib.py        # Script huáº¥n luyá»‡n DQN (cÅ©)
â”œâ”€â”€ src/environment/drl_algo/
â”‚   â”œâ”€â”€ env.py                # SumoEnvironment (khÃ´ng thay Ä‘á»•i)
â”‚   â”œâ”€â”€ traffic_signal.py     # TrafficSignal class (khÃ´ng thay Ä‘á»•i)
â”‚   â”œâ”€â”€ observations.py       # Observation functions (khÃ´ng thay Ä‘á»•i)
â”‚   â””â”€â”€ resco_envs.py         # RESCO environments (khÃ´ng thay Ä‘á»•i)
â”œâ”€â”€ network/
â”‚   â”œâ”€â”€ grid4x4/              # Grid 4x4 network
â”‚   â”œâ”€â”€ 4x4loop/              # 4x4 Loop network
â”‚   â”œâ”€â”€ zurich/               # Zurich network
â”‚   â””â”€â”€ PhuQuoc/              # Phu Quoc network
â”œâ”€â”€ pyproject.toml            # Dependencies (khÃ´ng thay Ä‘á»•i)
â””â”€â”€ results/                  # ThÆ° má»¥c lÆ°u káº¿t quáº£ (táº¡o tá»± Ä‘á»™ng)
    â””â”€â”€ ppo_grid4x4_20250104_120000/
        â”œâ”€â”€ checkpoint_000010/
        â”œâ”€â”€ checkpoint_000020/
        â””â”€â”€ training_config.json
```

---

## ğŸ”§ CÃ i Ä‘áº·t MÃ´i trÆ°á»ng

### 1. Táº¡o vÃ  KÃ­ch hoáº¡t MÃ´i trÆ°á»ng áº¢o

```bash
# Táº¡o mÃ´i trÆ°á»ng áº£o
python3 -m venv .venv

# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng (Linux/macOS)
source .venv/bin/activate

# KÃ­ch hoáº¡t mÃ´i trÆ°á»ng (Windows)
.venv\Scripts\activate
```

### 2. CÃ i Ä‘áº·t Dependencies

```bash
# NÃ¢ng cáº¥p pip, setuptools, wheel
pip install --upgrade pip setuptools wheel

# CÃ i Ä‘áº·t cÃ¡c dependencies tá»« pyproject.toml
pip install -e .

# Hoáº·c cÃ i tá»«ng package
pip install "pettingzoo>=1.25.0"
pip install "gymnasium>=1.1.1"
pip install "ray[rllib]>=2.50.1"
pip install "numpy>=1.24.0"
pip install "pyvirtualdisplay>=3.0"
```

### 3. Kiá»ƒm tra CÃ i Ä‘áº·t

```bash
python -c "import ray; import gymnasium; print('âœ“ Installation successful')"
```

### 4. Äáº·t Biáº¿n MÃ´i trÆ°á»ng SUMO_HOME

```bash
# Linux/macOS
export SUMO_HOME=/usr/share/sumo
export PATH=$SUMO_HOME/bin:$PATH

# Hoáº·c thÃªm vÃ o ~/.bashrc hoáº·c ~/.zshrc
echo 'export SUMO_HOME=/usr/share/sumo' >> ~/.bashrc
echo 'export PATH=$SUMO_HOME/bin:$PATH' >> ~/.bashrc
source ~/.bashrc
```

---

## ğŸš€ Huáº¥n luyá»‡n MÃ´ hÃ¬nh

### CÃ¡ch 1: Sá»­ dá»¥ng Lá»‡nh ÄÆ¡n giáº£n

```bash
# Huáº¥n luyá»‡n vá»›i cÃ i Ä‘áº·t máº·c Ä‘á»‹nh (grid4x4, 100 iterations)
python scripts/train_ppo.py

# Huáº¥n luyá»‡n network khÃ¡c
python scripts/train_ppo.py --network zurich

# Huáº¥n luyá»‡n 500 iterations vá»›i 4 workers
python scripts/train_ppo.py --network grid4x4 --iterations 500 --workers 4

# Huáº¥n luyá»‡n vá»›i GPU
python scripts/train_ppo.py --gpu

# Huáº¥n luyá»‡n vá»›i SUMO GUI
python scripts/train_ppo.py --gui
```

### CÃ¡ch 2: Xem Äáº§y Ä‘á»§ cÃ¡c Tham sá»‘

```bash
python scripts/train_ppo.py --help
```

**Output:**
```
usage: train_ppo.py [-h] [--network {grid4x4,4x4loop,network_test,zurich,PhuQuoc}]
                     [--iterations ITERATIONS]
                     [--workers WORKERS]
                     [--checkpoint-interval CHECKPOINT_INTERVAL]
                     [--reward-threshold REWARD_THRESHOLD]
                     [--experiment-name EXPERIMENT_NAME]
                     [--gui]
                     [--gpu]
                     [--seed SEED]
                     [--output-dir OUTPUT_DIR]

optional arguments:
  -h, --help            show this help message and exit
  --network {grid4x4,4x4loop,network_test,zurich,PhuQuoc}
                        Network name (default: grid4x4)
  --iterations ITERATIONS
                        Number of training iterations (default: 100)
  --workers WORKERS     Number of parallel workers (default: 2)
  --checkpoint-interval CHECKPOINT_INTERVAL
                        Checkpoint interval (default: 10)
  --reward-threshold REWARD_THRESHOLD
                        Stop training if reward exceeds this threshold
  --experiment-name EXPERIMENT_NAME
                        Experiment name for logging
  --gui                 Use SUMO GUI
  --gpu                 Use GPU for training
  --seed SEED           Random seed (default: 42)
  --output-dir OUTPUT_DIR
                        Output directory for results (default: ./results)
```

### CÃ¡ch 3: VÃ­ dá»¥ Thá»±c táº¿

```bash
# Huáº¥n luyá»‡n thá»­ nghiá»‡m nhanh (10 iterations, 1 worker)
python scripts/train_ppo.py \
  --network grid4x4 \
  --iterations 10 \
  --workers 1 \
  --checkpoint-interval 5 \
  --seed 123

# Huáº¥n luyá»‡n sáº£n xuáº¥t (500 iterations, 4 workers, GPU)
python scripts/train_ppo.py \
  --network zurich \
  --iterations 500 \
  --workers 4 \
  --checkpoint-interval 20 \
  --gpu \
  --seed 42 \
  --experiment-name "ppo_zurich_final"

# Huáº¥n luyá»‡n vá»›i ngÆ°á»¡ng dá»«ng (stop khi reward > 100)
python scripts/train_ppo.py \
  --network 4x4loop \
  --iterations 1000 \
  --reward-threshold 100 \
  --workers 4
```

---

## ğŸ“Š ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh

### Sau Huáº¥n luyá»‡n

```bash
# Láº¥y Ä‘Æ°á»ng dáº«n checkpoint tá»« results
CHECKPOINT_PATH="./results/ppo_grid4x4_20250104_120000/checkpoint_000050"

# ÄÃ¡nh giÃ¡ 5 episodes
python scripts/eval_ppo.py --checkpoint $CHECKPOINT_PATH

# ÄÃ¡nh giÃ¡ 10 episodes trÃªn network khÃ¡c
python scripts/eval_ppo.py \
  --checkpoint $CHECKPOINT_PATH \
  --network zurich \
  --episodes 10

# ÄÃ¡nh giÃ¡ vá»›i SUMO GUI
python scripts/eval_ppo.py \
  --checkpoint $CHECKPOINT_PATH \
  --gui \
  --episodes 3
```

---

## âš™ï¸ Tham sá»‘ Cáº¥u hÃ¬nh

### Tham sá»‘ SUMO Environment

| Tham sá»‘ | GiÃ¡ trá»‹ Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|-----------------|-------|
| `max_green` | 60s | Thá»i gian xanh tá»‘i Ä‘a |
| `min_green` | 5s | Thá»i gian xanh tá»‘i thiá»ƒu |
| `delta_time` | 5s | Khoáº£ng thá»i gian giá»¯a cÃ¡c action |
| `yellow_time` | 3s | Thá»i gian vÃ ng |
| `use_gui` | False | Sá»­ dá»¥ng SUMO GUI |

### Tham sá»‘ PPO Training

| Tham sá»‘ | GiÃ¡ trá»‹ Máº·c Ä‘á»‹nh | MÃ´ táº£ |
|---------|-----------------|-------|
| `learning_rate` | 5e-5 | Tá»‘c Ä‘á»™ há»c |
| `gamma` | 0.99 | Discount factor |
| `lambda_` | 0.95 | GAE lambda |
| `entropy_coeff` | 0.01 | Há»‡ sá»‘ entropy (exploration) |
| `clip_param` | 0.3 | PPO clip parameter |
| `fcnet_hiddens` | [256, 256] | KÃ­ch thÆ°á»›c hidden layers |
| `sgd_minibatch_size` | 128 | Batch size cho SGD |
| `train_batch_size` | 4096 | Tá»•ng batch size cho training |
| `num_sgd_iter` | 30 | Sá»‘ láº§n update SGD má»—i iteration |

### Äiá»u chá»‰nh Tham sá»‘

Äá»ƒ thay Ä‘á»•i tham sá»‘ PPO, chá»‰nh sá»­a hÃ m `create_ppo_config()` trong `train_ppo.py`:

```python
def create_ppo_config(...):
    config = (
        PPOConfig()
        ...
        .training(
            lr=1e-4,              # â† TÄƒng learning rate
            entropy_coeff=0.02,   # â† TÄƒng exploration
            clip_param=0.2,       # â† Giáº£m clip param (conservative)
            ...
        )
    )
```

---

## ğŸ§  Cáº¥u trÃºc PPO

### Thuáº­t toÃ¡n PPO (Proximal Policy Optimization)

**Æ¯u Ä‘iá»ƒm:**
- âœ… á»”n Ä‘á»‹nh hÆ¡n policy gradient methods
- âœ… Hiá»‡u quáº£ sample-efficient
- âœ… Dá»… implement vÃ  debug
- âœ… Hoáº¡t Ä‘á»™ng tá»‘t vá»›i both discrete vÃ  continuous actions

**CÃ´ng thá»©c Update:**

```
L^CLIP(Î¸) = E_t [ min(r_t(Î¸) * Ã‚_t, clip(r_t(Î¸), 1-Îµ, 1+Îµ) * Ã‚_t) ]

Trong Ä‘Ã³:
  r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (Probability Ratio)
  Ã‚_t = Q(s_t, a_t) - V(s_t)               (Advantage Estimate)
  Îµ = clip_param = 0.3                      (Clip range)
```

**Luá»“ng Huáº¥n luyá»‡n:**

```
1. Rollout phase:
   - Cháº¡y environment vá»›i current policy
   - Láº¥y {s_t, a_t, r_t, s_{t+1}}
   - TÃ­nh advantages báº±ng GAE

2. Update phase (multiple epochs):
   - Shuffle trajectories
   - Mini-batch updates
   - Calculate PPO loss
   - Backward pass
   - Update parameters

3. Repeat Ä‘áº¿n khi converge hoáº·c max iterations
```

### Network Architecture

```
State Input (Observation)
    â†“
Linear Layer (256, relu)
    â†“
Linear Layer (256, relu)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â†“                 â†“
Policy Head   Value Head
(Actor)       (Critic)
    â†“             â†“
Output: Ï€Î¸    Output: V(s)
(Action)      (State Value)
```

---

## ğŸ“ˆ Káº¿t quáº£ Huáº¥n luyá»‡n

### Cáº¥u trÃºc ThÆ° má»¥c Káº¿t quáº£

```
results/
â””â”€â”€ ppo_grid4x4_20250104_120000/
    â”œâ”€â”€ checkpoint_000010/
    â”‚   â”œâ”€â”€ algorithm_state.pkl
    â”‚   â”œâ”€â”€ policy_0
    â”‚   â”‚   â”œâ”€â”€ model.pkl
    â”‚   â”‚   â””â”€â”€ rllib_checkpoint.json
    â”‚   â”œâ”€â”€ rllib_checkpoint.json
    â”‚   â””â”€â”€ training_iteration
    â”œâ”€â”€ checkpoint_000020/
    â”œâ”€â”€ checkpoint_000030/
    â”œâ”€â”€ training_config.json
    â””â”€â”€ progress.csv
```

### File Log

**training_config.json**: LÆ°u cáº¥u hÃ¬nh training
```json
{
  "experiment_name": "ppo_grid4x4_20250104_120000",
  "network_name": "grid4x4",
  "num_iterations": 100,
  "num_workers": 2,
  "checkpoint_interval": 10,
  "use_gpu": false,
  "seed": 42,
  "best_checkpoint": "./results/ppo_grid4x4_20250104_120000/checkpoint_000050",
  "best_reward": 125.45
}
```

### Theo dÃµi Tiáº¿n Ä‘á»™

Khi training, báº¡n sáº½ tháº¥y output nhÆ°:

```
Iteration   1 | Episode Reward Mean:   -12.45 | Episode Len Mean:   250.0
Iteration   2 | Episode Reward Mean:   -10.32 | Episode Len Mean:   255.3
Iteration   3 | Episode Reward Mean:     5.67 | Episode Len Mean:   265.1
Iteration   4 | Episode Reward Mean:    18.90 | Episode Len Mean:   268.5
...
Iteration 100 | Episode Reward Mean:   125.45 | Episode Len Mean:   285.2

âœ“ Training completed: reached max iterations (100)
```

---

## ğŸ” Troubleshooting

### Lá»—i: "SUMO_HOME not set"

```bash
# Kiá»ƒm tra SUMO_HOME
echo $SUMO_HOME

# Náº¿u rá»—ng, Ä‘áº·t biáº¿n mÃ´i trÆ°á»ng
export SUMO_HOME=/usr/share/sumo
```

### Lá»—i: "Out of Memory"

```bash
# Giáº£m sá»‘ workers
python scripts/train_ppo.py --workers 1

# Hoáº·c giáº£m batch size trong code
# (sá»­a sgd_minibatch_size, train_batch_size)
```

### Training Cháº­m

```bash
# TÄƒng sá»‘ workers (náº¿u Ä‘á»§ CPU)
python scripts/train_ppo.py --workers 8

# Hoáº·c sá»­ dá»¥ng GPU
python scripts/train_ppo.py --gpu
```

### MÃ´ hÃ¬nh KhÃ´ng Há»c

- âœ“ Kiá»ƒm tra environment cÃ³ reward khÃ´ng
- âœ“ TÄƒng learning_rate hoáº·c entropy_coeff
- âœ“ Giáº£m clip_param (máº·c Ä‘á»‹nh 0.3)
- âœ“ Kiá»ƒm tra láº¡i initialization

---

## ğŸ“š TÃ i liá»‡u Tham kháº£o

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Ray RLlib Docs](https://docs.ray.io/en/latest/rllib/index.html)
- [SUMO Documentation](https://sumo.dlr.de/docs/)
- [Gymnasium Docs](https://gymnasium.farama.org/)

---

## ğŸ“ Ghi ChÃº

- Script nÃ y khÃ´ng thay Ä‘á»•i code trong `src/environment/drl_algo/`
- MÃ´ hÃ¬nh Ä‘Æ°á»£c lÆ°u dÆ°á»›i dáº¡ng checkpoint cÃ³ thá»ƒ load láº¡i Ä‘á»ƒ tiáº¿p tá»¥c training
- Má»—i láº§n training Ä‘Æ°á»£c táº¡o má»™t thÆ° má»¥c riÃªng vá»›i timestamp Ä‘á»ƒ trÃ¡nh ghi Ä‘Ã¨
- Báº¡n cÃ³ thá»ƒ cháº¡y nhiá»u training song song trÃªn cÃ¡c worker khÃ¡c nhau

---

TÃ¡c giáº£: AI for Traffic Signal Control  
NgÃ y: 2025
